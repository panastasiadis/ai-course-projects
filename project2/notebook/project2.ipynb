{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset\n",
        "Mount Google Drive and make the dataset files accesible from the notebook."
      ],
      "metadata": {
        "id": "W9HfygRc-A2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJM6cqwNu2Ys",
        "outputId": "b6527e19-e70f-4cf3-e709-7977c6ee743a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# adjust the paths accordingly\n",
        "# comment the following out if you are in local mode\n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/project2-test\")\n",
        "drive_path = \"/content/drive/MyDrive/Colab Notebooks/project2-test\"\n",
        "\n",
        "# -- uncomment the following if you are in local mode\n",
        "# drive_path = \".\"\n",
        "# -- make sure to have the \"dataset2\" folder in your working directory as well the my-photos folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF4Pm_JuWkw"
      },
      "source": [
        "### Import the necessary libraries\n",
        "Python libraries used below in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import os\n",
        "import shutil\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import metrics\n"
      ],
      "metadata": {
        "id": "3TPkTLJd9wzF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezzYAH6OuWk_"
      },
      "source": [
        "### Save dataset pictures with proper structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J1OZe668zP28"
      },
      "outputs": [],
      "source": [
        "# Setting up file paths for the dataset\n",
        "data_path = drive_path + \"/\" + \"data\"\n",
        "sunrise_path = drive_path + \"/\" + \"data/sunrise\"\n",
        "shine_path = drive_path + \"/\" + \"data/shine\"\n",
        "cloudy_path = drive_path + \"/\" + \"data/cloudy\"\n",
        "rain_path = drive_path + \"/\" + \"data/rain\"\n",
        "images_dict = utils.get_dict_with_files_per_class(drive_path + \"/\" \"dataset2\")\n",
        "\n",
        "# the path for my custom photos\n",
        "myphotos_path = drive_path + \"/\" + \"my-photos\"\n",
        "\n",
        "def store_in_keras_structure():\n",
        "\n",
        "  \"\"\"\n",
        "    Function to organize images into a structure suitable \n",
        "    for training a Keras model.\n",
        "    It creates necessary directories and \n",
        "    moves images to their respective class directories.\n",
        "  \"\"\"\n",
        "    \n",
        "  # Create necessary directories if they don't exist\n",
        "  if not(os.path.isdir(data_path)):\n",
        "    os.mkdir(data_path)\n",
        "    os.mkdir(shine_path)\n",
        "    os.mkdir(rain_path)\n",
        "    os.mkdir(sunrise_path)\n",
        "    os.mkdir(cloudy_path)\n",
        "\n",
        "    # Move and rename images into their respective class directories\n",
        "\n",
        "    # Handling sunrise class\n",
        "    i = 1\n",
        "    for image in images_dict['sunrise']:\n",
        "      new_filename = \"sunrise_image_{0}.jpg\".format(i)\n",
        "      shutil.move(image, sunrise_path + \"/\" + new_filename)\n",
        "      i += 1\n",
        "    \n",
        "    # Handling cloudy class\n",
        "    i = 1\n",
        "    for image in images_dict['cloudy']:\n",
        "        new_filename = \"cloudy_image_{0}.jpg\".format(i)\n",
        "        shutil.move(image, cloudy_path + \"/\" + new_filename)\n",
        "        i += 1\n",
        "    \n",
        "    # Handling rain class\n",
        "    i = 1\n",
        "    for image in images_dict['rain']:\n",
        "        new_filename = \"rain_image_{0}.jpg\".format(i)\n",
        "        shutil.move(image, rain_path + \"/\" + new_filename)\n",
        "        i += 1\n",
        "    \n",
        "    # Handling shine class\n",
        "    i = 1\n",
        "    for image in images_dict['shine']:\n",
        "        new_filename = \"shine_image_{0}.jpg\".format(i)\n",
        "        shutil.move(image, shine_path + \"/\" + new_filename)\n",
        "        i += 1\n",
        "        \n",
        "store_in_keras_structure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO6jR-RkuWlH"
      },
      "source": [
        "### Create train, validation and test subsets.\n",
        "\n",
        "Using image_dataset_from_directory from Keras to load the data. We create a 60% train subset, a 20% validation subset and a 20% test subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oflYVideuWlJ",
        "outputId": "98c6adf8-ca20-46df-caa6-6b46640233c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1125 files belonging to 4 classes.\n",
            "Using 675 files for training.\n",
            "Found 1125 files belonging to 4 classes.\n",
            "Using 450 files for validation.\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "# Select a fixed size (256, 256) for the loaded images\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "# Load the training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.4,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    seed=1,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Load the test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.4,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    seed=1,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Determine the number of validation batches\n",
        "val_batches = tf.data.experimental.cardinality(test_ds)\n",
        "print(val_batches)\n",
        "\n",
        "# Create a validation dataset by taking the first half of the test dataset\n",
        "val_ds = test_ds.take((1*val_batches) // 2)\n",
        "# Update the test dataset to skip the first half and keep the second half\n",
        "test_ds = test_ds.skip((1*val_batches) // 2)\n",
        "\n",
        "# Get the number of classes\n",
        "NUM_CLASSES = len(train_ds.class_names)\n",
        "print(NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXYgLZMuWlL"
      },
      "source": [
        "# Create a metrics function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ***confusion_matrix*** function calculates the confusion matrix for the requested subset."
      ],
      "metadata": {
        "id": "5X4vUwQsCklc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L8CTZf0FuWlM"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(model, subset):\n",
        "\n",
        "    # Set the appropriate dataset based on the subset\n",
        "    if subset==\"test\":\n",
        "        ds = test_ds\n",
        "    elif subset==\"val\":\n",
        "        ds = val_ds\n",
        "    elif subset==\"train\":\n",
        "        ds = train_ds\n",
        "    elif subset==\"myphotos\":\n",
        "       ds = myphotos_ds\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    # x: for every iteration, x contains a batch of 64 images (256,256), RGB \n",
        "    # y: for every iteration, y contains the corresponding labels (one_hot) for each batch\n",
        "    for x, y in ds:\n",
        "        # argmax and axis = 1 go through every one-hot combination \n",
        "        # and return the index with the highest value   \n",
        "        y = tf.argmax(y, axis=1)\n",
        "        # append the result to the array with the true labels\n",
        "        y_true.append(y)\n",
        "        # do the same for the predicted labels        \n",
        "        pred = model.predict(x)\n",
        "        y_pred.append(tf.argmax(pred, axis=1))\n",
        "    \n",
        "    # we concatenate the batches into one \n",
        "    y_pred = tf.concat(y_pred, axis=0)\n",
        "    y_true = tf.concat(y_true, axis=0)\n",
        "\n",
        "    # compute and return the confusion matrix\n",
        "    return metrics.confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ***calculate_metrics*** function takes as input the confusion matrix and then calculates and finally prints both the overall and per class evaluation metrics (accuracy, precision, recall and f1-score)."
      ],
      "metadata": {
        "id": "robBR7k5DL1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(cm):\n",
        "  # Confusion Matrix\n",
        "  print(\"-----------------------------\")\n",
        "  print(cm)\n",
        "  print(\"-----------------------------\")\n",
        "  # Overall Accuracy\n",
        "  total_samples = sum(sum(row) for row in cm)\n",
        "  accuracy = (cm[0][0] + cm[1][1] + cm[2][2] + cm[3][3]) / total_samples\n",
        "  print(\"Overall Accuracy:\", accuracy)\n",
        "\n",
        "  num_classes = len(cm)\n",
        "  precision = []\n",
        "  recall = []\n",
        "  f1_score = []\n",
        "  class_accuracy = []\n",
        "\n",
        "  # Calculate accuracy, precision, recall, and F1-score for each class\n",
        "  for i in range(num_classes):\n",
        "    class_total = sum(cm[i])  # Total samples for the class\n",
        "    class_correct = cm[i][i]  # Correctly classified samples for the class\n",
        "    class_accuracy.append(class_correct / class_total)\n",
        "\n",
        "    overall_tp = sum(cm[i][i] for i in range(num_classes))\n",
        "    overall_fp = sum(sum(cm[j][i] for j in range(num_classes) if j != i) for i in range(num_classes))\n",
        "    overall_fn = sum(sum(cm[i][j] for j in range(num_classes) if j != i) for i in range(num_classes))\n",
        "    \n",
        "    # True positives for class i\n",
        "    tp = cm[i][i]  \n",
        "    # False positives for class i\n",
        "    fp = sum(cm[j][i] for j in range(num_classes) if j != i)\n",
        "    # False negatives for class i  \n",
        "    fn = sum(cm[i][j] for j in range(num_classes) if j != i)\n",
        "\n",
        "    # Precision for class i\n",
        "    precision.append(tp / (tp + fp) if tp + fp > 0 else 0)\n",
        "    # Recall for class i  \n",
        "    recall.append(tp / (tp + fn) if tp + fn > 0 else 0)  \n",
        "    # F1-score for class i\n",
        "    f1_score.append((2 * precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0)\n",
        "\n",
        "  # Calculate overall precision, recall, and F1-score\n",
        "  overall_precision = overall_tp / (overall_tp + overall_fp) if (overall_tp + overall_fp) > 0 else 0\n",
        "  overall_recall = overall_tp / (overall_tp + overall_fn) if (overall_tp + overall_fn) > 0 else 0\n",
        "  overall_f1_score = (2 * overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
        "\n",
        "  # Print overall precision, recall, and F1-score\n",
        "  print(\"Overall Precision:\", round(overall_precision, 5))\n",
        "  print(\"Overall Recall:\", round(overall_recall, 5))\n",
        "  print(\"Overall F1-score:\", round(overall_f1_score, 5))\n",
        "  print(\"-----------------------------\")\n",
        "  # Print accuracy, precision, recall, and F1-score per class\n",
        "  print(\"Accuracy per class:\", [f\"{val:.5f}\" for val in class_accuracy])\n",
        "  print(\"Precision per class:\", [f\"{val:.5f}\" for val in precision])\n",
        "  print(\"Recall per class:\", [f\"{val:.5f}\" for val in recall])\n",
        "  print(\"F1 score per class:\", [f\"{val:.5f}\" for val in f1_score])\n",
        "  print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "hJHZw_ukZJuw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ***model_custom_eval*** function takes as input a given model and then calculates the metrics for all the model's subsets (train, validation and test)."
      ],
      "metadata": {
        "id": "bb1ISChAEEIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_custom_eval(model): \n",
        "  # Calculate confusion matrices for train, val, and test sets\n",
        "  cm_train = confusion_matrix(model, \"train\")\n",
        "  cm_val = confusion_matrix(model, \"val\")\n",
        "  cm_test = confusion_matrix(model, \"test\")\n",
        "  \n",
        "  # Print evaluation results\n",
        "  print(\"<========== Train ==========>\")\n",
        "  calculate_metrics(cm_train)\n",
        "  print(\"<========== Val ============>\")\n",
        "  calculate_metrics(cm_val)\n",
        "  print(\"<========== Test ===========>\")\n",
        "  calculate_metrics(cm_test)\n",
        "  print(\"<===========================>\")"
      ],
      "metadata": {
        "id": "3AI1vUUWzD4D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbVVfUQNuWlO"
      },
      "source": [
        "# Create a simple CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "We implement a simple CNN model architecture that follows the project guidelines."
      ],
      "metadata": {
        "id": "38Kz71yVE1N7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss-HOTucuWlO"
      },
      "outputs": [],
      "source": [
        "def cnn_simple(num_classes):\n",
        "    return tf.keras.Sequential(\n",
        "        [\n",
        "            # Rescale pixel values to [0, 1]\n",
        "            tf.keras.layers.Rescaling(1.0 / 255),\n",
        "            \n",
        "            tf.keras.layers.Conv2D(8, 3, padding=\"same\",activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D(strides=(2,2)),\n",
        "            tf.keras.layers.Conv2D(16, 3, padding=\"same\",activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D(strides=(2,2)),\n",
        "            # Flatten the feature maps\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "We compile the model using, again, the project guidelines for the various parameters."
      ],
      "metadata": {
        "id": "2RxzAa5hGie1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_simple_model = cnn_simple(NUM_CLASSES)\n",
        "\n",
        "cnn_simple_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "jWZREc4WGZ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model and use early stopping if there is no further decrease of validation loss after 5 epochs."
      ],
      "metadata": {
        "id": "E9dJOcIZNEd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "cnn_simple_model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=20,\n",
        "  callbacks=[early_stop_cb]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDfsdXpANDpx",
        "outputId": "484973e5-14c0-40fe-c6ab-9d56cdd16e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 289s 13s/step - loss: 3.6535 - accuracy: 0.2726 - val_loss: 1.2842 - val_accuracy: 0.4688\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 6s 368ms/step - loss: 1.1427 - accuracy: 0.5304 - val_loss: 1.0782 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 5s 242ms/step - loss: 0.9327 - accuracy: 0.6415 - val_loss: 0.9128 - val_accuracy: 0.6523\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 5s 340ms/step - loss: 0.8083 - accuracy: 0.6904 - val_loss: 0.7943 - val_accuracy: 0.6758\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 7s 336ms/step - loss: 0.6971 - accuracy: 0.6889 - val_loss: 0.7069 - val_accuracy: 0.6758\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 4s 236ms/step - loss: 0.6127 - accuracy: 0.7067 - val_loss: 0.6616 - val_accuracy: 0.6836\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 6s 361ms/step - loss: 0.5268 - accuracy: 0.7393 - val_loss: 0.5898 - val_accuracy: 0.7461\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 4s 241ms/step - loss: 0.4490 - accuracy: 0.7852 - val_loss: 0.5519 - val_accuracy: 0.8203\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 6s 371ms/step - loss: 0.3528 - accuracy: 0.8815 - val_loss: 0.5812 - val_accuracy: 0.8672\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 5s 339ms/step - loss: 0.2812 - accuracy: 0.9230 - val_loss: 0.3618 - val_accuracy: 0.9102\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 5s 310ms/step - loss: 0.2343 - accuracy: 0.9259 - val_loss: 0.3074 - val_accuracy: 0.9102\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 4s 242ms/step - loss: 0.1982 - accuracy: 0.9304 - val_loss: 0.4632 - val_accuracy: 0.8867\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 6s 328ms/step - loss: 0.2067 - accuracy: 0.9215 - val_loss: 0.5033 - val_accuracy: 0.8945\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 4s 238ms/step - loss: 0.1595 - accuracy: 0.9541 - val_loss: 0.3323 - val_accuracy: 0.9141\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 5s 338ms/step - loss: 0.1481 - accuracy: 0.9496 - val_loss: 0.3465 - val_accuracy: 0.8828\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 6s 341ms/step - loss: 0.1533 - accuracy: 0.9467 - val_loss: 0.3577 - val_accuracy: 0.9023\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a99936ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dHwNTHGuWlP"
      },
      "source": [
        "### Evaluate\n",
        "We use our custom evaluation function to evaluate the model and display the necessary metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kltcFNVUuWlP",
        "outputId": "63497c23-cee2-4bc9-8056-f8af8808e105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 15ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 77ms/step\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[175   0  13   0]\n",
            " [  7 122   0   0]\n",
            " [ 11   0 129   0]\n",
            " [  1   0   2 215]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9496296296296296\n",
            "Overall Precision: 0.94963\n",
            "Overall Recall: 0.94963\n",
            "Overall F1-score: 0.94963\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.93085', '0.94574', '0.92143', '0.98624']\n",
            "Precision per class: ['0.90206', '1.00000', '0.89583', '1.00000']\n",
            "Recall per class: ['0.93085', '0.94574', '0.92143', '0.98624']\n",
            "F1 score per class: ['0.91623', '0.97211', '0.90845', '0.99307']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[57  1  1  1]\n",
            " [ 8 42  1  0]\n",
            " [ 7  3 63  0]\n",
            " [ 0  0  1 71]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.91015625\n",
            "Overall Precision: 0.91016\n",
            "Overall Recall: 0.91016\n",
            "Overall F1-score: 0.91016\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.95000', '0.82353', '0.86301', '0.98611']\n",
            "Precision per class: ['0.79167', '0.91304', '0.95455', '0.98611']\n",
            "Recall per class: ['0.95000', '0.82353', '0.86301', '0.98611']\n",
            "F1 score per class: ['0.86364', '0.86598', '0.90647', '0.98611']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[48  0  2  0]\n",
            " [ 2 36  1  0]\n",
            " [ 7  1 37  1]\n",
            " [ 2  0  1 56]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9123711340206185\n",
            "Overall Precision: 0.91237\n",
            "Overall Recall: 0.91237\n",
            "Overall F1-score: 0.91237\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.96000', '0.92308', '0.80435', '0.94915']\n",
            "Precision per class: ['0.81356', '0.97297', '0.90244', '0.98246']\n",
            "Recall per class: ['0.96000', '0.92308', '0.80435', '0.94915']\n",
            "F1 score per class: ['0.88073', '0.94737', '0.85057', '0.96552']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(cnn_simple_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "The CNN model achieved excellent accuracy on the training set (**94.96%**) as well as on the validation and test subsets (**~91%**).\n",
        "\n",
        "Class specific analysis revealed slightly lower accuracy for the \"rain\" class on the validation subset (0.82) and for the \"shine\" class on the test subset (0.80) \n"
      ],
      "metadata": {
        "id": "A7R827BUYJal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # Create a CNN model of bigger depth"
      ],
      "metadata": {
        "id": "H6aix9_bN7vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "We implement the more complex CNN model architecture following the project guidelines."
      ],
      "metadata": {
        "id": "AaA90JHMODjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnN9jtqYuWlQ"
      },
      "outputs": [],
      "source": [
        "def cnn_complex(num_classes):\n",
        "    return tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Rescaling(1.0 / 255),\n",
        "            tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D(strides=(4, 4)),\n",
        "            tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(64, 3, padding=\"same\",activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D(strides=(2, 2)),\n",
        "            tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "            tf.keras.layers.Conv2D(128, 3, padding=\"same\",activation=\"relu\"),\n",
        "            tf.keras.layers.MaxPooling2D(strides=(2, 2)),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "We compile the model following the project instructions and use the same parameters as before."
      ],
      "metadata": {
        "id": "B0U003XLOYUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_complex_model = cnn_complex(NUM_CLASSES)\n",
        "\n",
        "cnn_complex_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "metadata": {
        "id": "L5-TCc_BOSgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model using early stoppping after 5 epochs. "
      ],
      "metadata": {
        "id": "9JNaueOdO2GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "cnn_complex_model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=[early_stop_cb])"
      ],
      "metadata": {
        "id": "RB3WN17dOzbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158d7d6d-b364-4405-bf51-49004c0a46b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 12s 623ms/step - loss: 1.4050 - accuracy: 0.3126 - val_loss: 1.2287 - val_accuracy: 0.2930\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 7s 493ms/step - loss: 1.0009 - accuracy: 0.5363 - val_loss: 1.0049 - val_accuracy: 0.6367\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 8s 520ms/step - loss: 0.8559 - accuracy: 0.6089 - val_loss: 0.7762 - val_accuracy: 0.6953\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 8s 602ms/step - loss: 0.6270 - accuracy: 0.7304 - val_loss: 0.5882 - val_accuracy: 0.7773\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 7s 475ms/step - loss: 0.6076 - accuracy: 0.7378 - val_loss: 0.5933 - val_accuracy: 0.7773\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 8s 502ms/step - loss: 0.5199 - accuracy: 0.7911 - val_loss: 0.5847 - val_accuracy: 0.7773\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 8s 556ms/step - loss: 0.4762 - accuracy: 0.8030 - val_loss: 0.5549 - val_accuracy: 0.8477\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 7s 481ms/step - loss: 0.4397 - accuracy: 0.8044 - val_loss: 0.3488 - val_accuracy: 0.8555\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 8s 478ms/step - loss: 0.3898 - accuracy: 0.8311 - val_loss: 0.6800 - val_accuracy: 0.8164\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 8s 564ms/step - loss: 0.5127 - accuracy: 0.8148 - val_loss: 0.5983 - val_accuracy: 0.8086\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 7s 486ms/step - loss: 0.4236 - accuracy: 0.8267 - val_loss: 0.4254 - val_accuracy: 0.8516\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 9s 484ms/step - loss: 0.3875 - accuracy: 0.8341 - val_loss: 0.4407 - val_accuracy: 0.8242\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 11s 818ms/step - loss: 0.4110 - accuracy: 0.8089 - val_loss: 0.6642 - val_accuracy: 0.7891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a8050d3f0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n",
        "We use our custom evaluation function to evaluate the complex model and display the evaluation metrics."
      ],
      "metadata": {
        "id": "_R3LvGR7PHPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMShk0PyuWlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2911dcde-0cd1-4a65-f14f-fe429106d6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 68ms/step\n",
            "2/2 [==============================] - 0s 49ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 49ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 47ms/step\n",
            "2/2 [==============================] - 0s 47ms/step\n",
            "2/2 [==============================] - 0s 57ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 40ms/step\n",
            "2/2 [==============================] - 0s 57ms/step\n",
            "2/2 [==============================] - 0s 47ms/step\n",
            "2/2 [==============================] - 0s 47ms/step\n",
            "2/2 [==============================] - 0s 45ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "2/2 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[170   6  12   0]\n",
            " [ 58  69   2   0]\n",
            " [ 19   1 120   0]\n",
            " [  4   0   4 210]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.8429629629629629\n",
            "Overall Precision: 0.84296\n",
            "Overall Recall: 0.84296\n",
            "Overall F1-score: 0.84296\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.90426', '0.53488', '0.85714', '0.96330']\n",
            "Precision per class: ['0.67729', '0.90789', '0.86957', '1.00000']\n",
            "Recall per class: ['0.90426', '0.53488', '0.85714', '0.96330']\n",
            "F1 score per class: ['0.77449', '0.67317', '0.86331', '0.98131']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[57  4  2  0]\n",
            " [29 23  0  1]\n",
            " [12  1 51  0]\n",
            " [ 3  0  2 71]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.7890625\n",
            "Overall Precision: 0.78906\n",
            "Overall Recall: 0.78906\n",
            "Overall F1-score: 0.78906\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.90476', '0.43396', '0.79688', '0.93421']\n",
            "Precision per class: ['0.56436', '0.82143', '0.92727', '0.98611']\n",
            "Recall per class: ['0.90476', '0.43396', '0.79688', '0.93421']\n",
            "F1 score per class: ['0.69512', '0.56790', '0.85714', '0.95946']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[48  2  3  0]\n",
            " [22 13  1  0]\n",
            " [10  1 40  1]\n",
            " [ 1  1  0 51]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.7835051546391752\n",
            "Overall Precision: 0.78351\n",
            "Overall Recall: 0.78351\n",
            "Overall F1-score: 0.78351\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.90566', '0.36111', '0.76923', '0.96226']\n",
            "Precision per class: ['0.59259', '0.76471', '0.90909', '0.98077']\n",
            "Recall per class: ['0.90566', '0.36111', '0.76923', '0.96226']\n",
            "F1 score per class: ['0.71642', '0.49057', '0.83333', '0.97143']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(cnn_complex_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "The complex model, despite having a more sophisticated architecture, does not perform as well as the simple model.\n",
        "\n",
        "The overall accuracy (84% on train, 78% on validation and test) is not as good as the simple model.\n",
        "\n",
        "Observing the per-class results, the \"rain\" class performs badly and achieves the lowest accuracy, precision and recall compared to the others.\n",
        "\n",
        "### Complex vs Simple Architecture\n",
        "\n",
        "For our dataset of 1152 images, the complex cnn model seems to have a larger capacity than necessary and it has difficulty capturing the essential features and patterns from the images.\n",
        "\n",
        "Another reason should be the training data quality, because the complex model seems to be more sensitive to incosistencies and misalabeled samples, especially considering the images belonging to the \"rain\" class or those that closely resemble \"rain\" images but actually belong to the other classes. "
      ],
      "metadata": {
        "id": "7ZkQ0l8ZidjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilize a pre-trained neural network \n",
        "For this task we select the InceptionV3 architecture."
      ],
      "metadata": {
        "id": "MQkj-DURPcGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "Documentation from Keras API on https://keras.io/api/applications/inceptionv3 indicates that we need to scale input pixels between -1 and 1 before we feed them to InceptionV3. This is done by using **inception_v3.preprocess_input**. "
      ],
      "metadata": {
        "id": "uZrFO1kuBJNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Function to preprocess images and labels\n",
        "def preprocess_func(images, labels):\n",
        "    return tf.keras.applications.inception_v3.preprocess_input(images), labels\n",
        "\n",
        "# Map the preprocessing function to the subsets\n",
        "train_ds = train_ds.map(preprocess_func)\n",
        "val_ds = val_ds.map(preprocess_func)\n",
        "test_ds = test_ds.map(preprocess_func)"
      ],
      "metadata": {
        "id": "TB8tsWkl7C0s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "\n",
        "We will utilize the InceptionV3 architecture in 4 ways.\n",
        "\n",
        "The following function provides the flexibility to toggle the \"freezing\" of the pretrained model and the inclusion of an additional dense layer with dropout.\n",
        "\n",
        "With the help of this function we can train and evaluate the model using 4 combinations and check the results."
      ],
      "metadata": {
        "id": "Tq9wePN_EeMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrained_inception_v3(num_classes, is_trainable, extra_layers_enable):\n",
        "  \n",
        "  # Load the pre-trained InceptionV3 model with weights from ImageNet\n",
        "  pretrained_model = InceptionV3(\n",
        "    input_shape=(299, 299, 3), include_top=False, weights=\"imagenet\"\n",
        ")\n",
        "\n",
        "  # Set the trainability of the pre-trained model based on the input parameter\n",
        "  pretrained_model.trainable = is_trainable \n",
        "\n",
        "  # Define the input layer for the model\n",
        "  input_layer = tf.keras.layers.Input(shape=(256, 256, 3))\n",
        "\n",
        "  # Resize the input images to match the input size of InceptionV3 (299x299)\n",
        "  # without aspect ratio distortion.\n",
        "  # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/smart_resize\n",
        "  resizing_layer = (\n",
        "    lambda image: tf.keras.preprocessing.image.smart_resize(image, (299, 299))\n",
        "  )(input_layer)\n",
        "\n",
        "  # Pass the resized images through the pre-trained InceptionV3 layers\n",
        "  inception_layers = pretrained_model(resizing_layer)\n",
        "\n",
        "  # Add a global spatial average pooling layer\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(inception_layers)\n",
        "\n",
        "  # Add extra fully-connected layers if enabled\n",
        "  if extra_layers_enable == True:\n",
        "    # add a fully-connected layer\n",
        "    layer_i = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(layer_i)\n",
        "\n",
        "  out_layer = tf.keras.layers.Dense(4, activation=\"softmax\")(x)\n",
        "\n",
        "  # Define the model with input and output layers\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=out_layer)\n",
        "  return model\n",
        "\n",
        "def train_inception_v3(num_classes, is_trainable, extra_layers_enable):\n",
        "  model = pretrained_inception_v3(num_classes, is_trainable=is_trainable, extra_layers_enable=extra_layers_enable)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "  )\n",
        "\n",
        "  early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "  model.fit(\n",
        "    train_ds, validation_data=val_ds, epochs=20, callbacks=[early_stop_cb]\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "pRXYzM6t7FPL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "7hK1eB-JE_gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try freezing the inception V3 and use the two extra layers (one Dense Layer with 1024 neurons(relu) and one Dropout Layer) "
      ],
      "metadata": {
        "id": "o3Px0SIWWPI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = train_inception_v3(NUM_CLASSES, False, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n15ceb7E-Vs",
        "outputId": "0719d56a-8b99-4d43-ba7b-d0c902461140"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 293s 25s/step - loss: 1.1121 - accuracy: 0.5200 - val_loss: 0.6590 - val_accuracy: 0.8086\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 262s 24s/step - loss: 0.5229 - accuracy: 0.8622 - val_loss: 0.3971 - val_accuracy: 0.9062\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 256s 24s/step - loss: 0.3527 - accuracy: 0.9052 - val_loss: 0.2920 - val_accuracy: 0.9297\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 264s 24s/step - loss: 0.2550 - accuracy: 0.9274 - val_loss: 0.2606 - val_accuracy: 0.9219\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 266s 25s/step - loss: 0.2261 - accuracy: 0.9422 - val_loss: 0.2521 - val_accuracy: 0.9180\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 265s 25s/step - loss: 0.1701 - accuracy: 0.9630 - val_loss: 0.2374 - val_accuracy: 0.9180\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 271s 25s/step - loss: 0.1504 - accuracy: 0.9615 - val_loss: 0.1851 - val_accuracy: 0.9336\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 263s 24s/step - loss: 0.1324 - accuracy: 0.9704 - val_loss: 0.1249 - val_accuracy: 0.9688\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 262s 24s/step - loss: 0.1137 - accuracy: 0.9778 - val_loss: 0.1764 - val_accuracy: 0.9492\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 265s 24s/step - loss: 0.1080 - accuracy: 0.9837 - val_loss: 0.1280 - val_accuracy: 0.9688\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 269s 25s/step - loss: 0.0920 - accuracy: 0.9837 - val_loss: 0.1776 - val_accuracy: 0.9375\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 250s 23s/step - loss: 0.0874 - accuracy: 0.9807 - val_loss: 0.1586 - val_accuracy: 0.9336\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 263s 24s/step - loss: 0.0779 - accuracy: 0.9867 - val_loss: 0.1483 - val_accuracy: 0.9570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing the inception V3 but not using the two extra layers."
      ],
      "metadata": {
        "id": "MRMgZXIcWx7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = train_inception_v3(NUM_CLASSES, False, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIzbB6MmW_2n",
        "outputId": "c2b910a1-d857-4588-b6cc-2f37e8ea727c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 16s 927ms/step - loss: 1.4492 - accuracy: 0.2830 - val_loss: 1.3516 - val_accuracy: 0.2969\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 10s 735ms/step - loss: 1.3064 - accuracy: 0.3526 - val_loss: 1.2550 - val_accuracy: 0.3906\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 7s 466ms/step - loss: 1.2108 - accuracy: 0.4400 - val_loss: 1.1701 - val_accuracy: 0.4688\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 8s 521ms/step - loss: 1.1256 - accuracy: 0.5259 - val_loss: 1.1102 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 7s 481ms/step - loss: 1.0485 - accuracy: 0.6074 - val_loss: 1.0254 - val_accuracy: 0.6133\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 8s 496ms/step - loss: 0.9781 - accuracy: 0.6770 - val_loss: 0.9696 - val_accuracy: 0.6328\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 10s 743ms/step - loss: 0.9173 - accuracy: 0.7289 - val_loss: 0.8867 - val_accuracy: 0.7344\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 7s 466ms/step - loss: 0.8603 - accuracy: 0.7585 - val_loss: 0.8381 - val_accuracy: 0.7500\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 7s 463ms/step - loss: 0.8100 - accuracy: 0.7807 - val_loss: 0.8337 - val_accuracy: 0.7812\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 11s 550ms/step - loss: 0.7649 - accuracy: 0.8089 - val_loss: 0.7361 - val_accuracy: 0.8398\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 8s 481ms/step - loss: 0.7245 - accuracy: 0.8444 - val_loss: 0.7151 - val_accuracy: 0.8398\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 8s 482ms/step - loss: 0.6884 - accuracy: 0.8489 - val_loss: 0.7127 - val_accuracy: 0.8438\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 8s 570ms/step - loss: 0.6542 - accuracy: 0.8637 - val_loss: 0.6539 - val_accuracy: 0.8633\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 7s 486ms/step - loss: 0.6241 - accuracy: 0.8711 - val_loss: 0.6254 - val_accuracy: 0.8906\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 8s 537ms/step - loss: 0.5966 - accuracy: 0.8844 - val_loss: 0.6066 - val_accuracy: 0.8789\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 10s 740ms/step - loss: 0.5706 - accuracy: 0.8904 - val_loss: 0.5763 - val_accuracy: 0.8984\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 7s 473ms/step - loss: 0.5463 - accuracy: 0.8948 - val_loss: 0.5280 - val_accuracy: 0.9219\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 7s 484ms/step - loss: 0.5246 - accuracy: 0.8978 - val_loss: 0.5181 - val_accuracy: 0.9141\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 10s 757ms/step - loss: 0.5044 - accuracy: 0.8993 - val_loss: 0.5329 - val_accuracy: 0.9023\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 7s 485ms/step - loss: 0.4855 - accuracy: 0.9052 - val_loss: 0.4855 - val_accuracy: 0.9141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable training of the inception V3 but not using the two extra layers."
      ],
      "metadata": {
        "id": "ERnG7YUB6oTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = train_inception_v3(NUM_CLASSES, True, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec0Hcjg0Y7kl",
        "outputId": "b8766388-7814-41cf-f9e5-c4ffe42cad66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 75s 2s/step - loss: 0.5879 - accuracy: 0.8000 - val_loss: 0.9128 - val_accuracy: 0.6641\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0326 - accuracy: 0.9941 - val_loss: 0.9955 - val_accuracy: 0.7109\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 0.6241 - val_accuracy: 0.8125\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.8242\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 17s 1s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8477\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3682 - val_accuracy: 0.8789\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.8906\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 9.3494e-04 - accuracy: 1.0000 - val_loss: 0.2725 - val_accuracy: 0.9180\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9375\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.1959 - val_accuracy: 0.9336\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.1680 - val_accuracy: 0.9492\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0909 - val_accuracy: 0.9570\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9570\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 0.0024 - accuracy: 0.9985 - val_loss: 0.0911 - val_accuracy: 0.9727\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 6.4056e-04 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 0.9727\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9727\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9844\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 2.2368e-04 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9844\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 2.9069e-04 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9766\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 13s 1s/step - loss: 7.3235e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable training of the inception V3 but use the two extra layers."
      ],
      "metadata": {
        "id": "wdPp0ucO6u4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = train_inception_v3(NUM_CLASSES, True, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrPSvEIHbAwo",
        "outputId": "d849e13b-61f2-4dd6-a731-4da14de012eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 53s 1s/step - loss: 0.7568 - accuracy: 0.7422 - val_loss: 0.7364 - val_accuracy: 0.6953\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0792 - accuracy: 0.9822 - val_loss: 0.7911 - val_accuracy: 0.7422\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.7843 - val_accuracy: 0.7695\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.8203\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4514 - val_accuracy: 0.8594\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.8906\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 4.9782e-04 - accuracy: 1.0000 - val_loss: 0.2844 - val_accuracy: 0.9141\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 6.0255e-04 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9102\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9570\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9688\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 4.2553e-04 - accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9805\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 6.3028e-04 - accuracy: 1.0000 - val_loss: 0.1470 - val_accuracy: 0.9570\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 4.5996e-04 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9727\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 4.6432e-04 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9688\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 14s 1s/step - loss: 6.1753e-04 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n",
        "\n",
        "Evaluating the 4 combinations of the models"
      ],
      "metadata": {
        "id": "GY3j-zfa6qQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(model1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-XVE9X0_rbH",
        "outputId": "0a461d80-97ed-4f23-b360-f15023c0c75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 4s 22ms/step\n",
            "2/2 [==============================] - 0s 158ms/step\n",
            "2/2 [==============================] - 0s 119ms/step\n",
            "2/2 [==============================] - 0s 118ms/step\n",
            "2/2 [==============================] - 0s 105ms/step\n",
            "2/2 [==============================] - 0s 108ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 108ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 2s 1s/step\n",
            "(array([0.99470899, 1.        , 1.        , 1.        ]), array([1.        , 0.99224806, 1.        , 1.        ]), array([0.99734748, 0.99610895, 1.        , 1.        ]), array([188, 129, 140, 218]))\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "(array([0.91803279, 1.        , 0.98507463, 0.96153846]), array([0.94915254, 0.98039216, 0.94285714, 0.98684211]), array([0.93333333, 0.99009901, 0.96350365, 0.97402597]), array([59, 51, 70, 76]))\n",
            "2/2 [==============================] - 0s 164ms/step\n",
            "2/2 [==============================] - 0s 115ms/step\n",
            "2/2 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "(array([1.        , 1.        , 0.91666667, 0.96825397]), array([0.90909091, 1.        , 1.        , 0.98387097]), array([0.95238095, 1.        , 0.95652174, 0.976     ]), array([55, 33, 44, 62]))\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[188   0   0   0]\n",
            " [  1 128   0   0]\n",
            " [  0   0 140   0]\n",
            " [  0   0   0 218]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9985185185185185\n",
            "Overall Precision: 0.99852\n",
            "Overall Recall: 0.99852\n",
            "Overall F1-score: 0.99852\n",
            "-----------------------------\n",
            "Accuracy per class: ['1.00000', '0.99225', '1.00000', '1.00000']\n",
            "Precision per class: ['0.99471', '1.00000', '1.00000', '1.00000']\n",
            "Recall per class: ['1.00000', '0.99225', '1.00000', '1.00000']\n",
            "F1 score per class: ['0.99735', '0.99611', '1.00000', '1.00000']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[56  0  1  2]\n",
            " [ 0 50  0  1]\n",
            " [ 4  0 66  0]\n",
            " [ 1  0  0 75]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.96484375\n",
            "Overall Precision: 0.96484\n",
            "Overall Recall: 0.96484\n",
            "Overall F1-score: 0.96484\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.94915', '0.98039', '0.94286', '0.98684']\n",
            "Precision per class: ['0.91803', '1.00000', '0.98507', '0.96154']\n",
            "Recall per class: ['0.94915', '0.98039', '0.94286', '0.98684']\n",
            "F1 score per class: ['0.93333', '0.99010', '0.96350', '0.97403']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[50  0  3  2]\n",
            " [ 0 33  0  0]\n",
            " [ 0  0 44  0]\n",
            " [ 0  0  1 61]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9690721649484536\n",
            "Overall Precision: 0.96907\n",
            "Overall Recall: 0.96907\n",
            "Overall F1-score: 0.96907\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.90909', '1.00000', '1.00000', '0.98387']\n",
            "Precision per class: ['1.00000', '1.00000', '0.91667', '0.96825']\n",
            "Recall per class: ['0.90909', '1.00000', '1.00000', '0.98387']\n",
            "F1 score per class: ['0.95238', '1.00000', '0.95652', '0.97600']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAnddrtBYFaU",
        "outputId": "400e0621-7aae-4dc7-e843-72c40cbdf246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 3s 161ms/step\n",
            "2/2 [==============================] - 0s 112ms/step\n",
            "2/2 [==============================] - 0s 114ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 108ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 1s 103ms/step\n",
            "(array([0.87234043, 0.96183206, 0.92622951, 0.8974359 ]), array([0.87234043, 0.97674419, 0.80714286, 0.96330275]), array([0.87234043, 0.96923077, 0.86259542, 0.92920354]), array([188, 129, 140, 218]))\n",
            "2/2 [==============================] - 0s 149ms/step\n",
            "2/2 [==============================] - 0s 114ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "(array([0.84313725, 0.96      , 0.93939394, 0.84269663]), array([0.74137931, 1.        , 0.86111111, 0.96153846]), array([0.78899083, 0.97959184, 0.89855072, 0.89820359]), array([58, 48, 72, 78]))\n",
            "2/2 [==============================] - 0s 158ms/step\n",
            "2/2 [==============================] - 0s 105ms/step\n",
            "2/2 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "(array([0.88      , 1.        , 0.94736842, 0.86153846]), array([0.84615385, 1.        , 0.81818182, 0.98245614]), array([0.8627451 , 1.        , 0.87804878, 0.91803279]), array([52, 41, 44, 57]))\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[164   3   6  15]\n",
            " [  1 126   1   1]\n",
            " [ 17   2 113   8]\n",
            " [  6   0   2 210]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9081481481481481\n",
            "Overall Precision: 0.90815\n",
            "Overall Recall: 0.90815\n",
            "Overall F1-score: 0.90815\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.87234', '0.97674', '0.80714', '0.96330']\n",
            "Precision per class: ['0.87234', '0.96183', '0.92623', '0.89744']\n",
            "Recall per class: ['0.87234', '0.97674', '0.80714', '0.96330']\n",
            "F1 score per class: ['0.87234', '0.96923', '0.86260', '0.92920']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[43  1  3 11]\n",
            " [ 0 48  0  0]\n",
            " [ 7  0 62  3]\n",
            " [ 1  1  1 75]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.890625\n",
            "Overall Precision: 0.89062\n",
            "Overall Recall: 0.89062\n",
            "Overall F1-score: 0.89062\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.74138', '1.00000', '0.86111', '0.96154']\n",
            "Precision per class: ['0.84314', '0.96000', '0.93939', '0.84270']\n",
            "Recall per class: ['0.74138', '1.00000', '0.86111', '0.96154']\n",
            "F1 score per class: ['0.78899', '0.97959', '0.89855', '0.89820']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[44  0  2  6]\n",
            " [ 0 41  0  0]\n",
            " [ 5  0 36  3]\n",
            " [ 1  0  0 56]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9123711340206185\n",
            "Overall Precision: 0.91237\n",
            "Overall Recall: 0.91237\n",
            "Overall F1-score: 0.91237\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.84615', '1.00000', '0.81818', '0.98246']\n",
            "Precision per class: ['0.88000', '1.00000', '0.94737', '0.86154']\n",
            "Recall per class: ['0.84615', '1.00000', '0.81818', '0.98246']\n",
            "F1 score per class: ['0.86275', '1.00000', '0.87805', '0.91803']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv0rci9vamWj",
        "outputId": "6ac3a868-533f-4b4c-c995-a9eedd87b4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 5s 25ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 105ms/step\n",
            "2/2 [==============================] - 0s 105ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 106ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 106ms/step\n",
            "2/2 [==============================] - 0s 107ms/step\n",
            "2/2 [==============================] - 2s 1s/step\n",
            "2/2 [==============================] - 0s 161ms/step\n",
            "2/2 [==============================] - 0s 102ms/step\n",
            "2/2 [==============================] - 0s 114ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 160ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[188   0   0   0]\n",
            " [  0 129   0   0]\n",
            " [  0   0 140   0]\n",
            " [  0   0   0 218]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 1.0\n",
            "Overall Precision: 1.0\n",
            "Overall Recall: 1.0\n",
            "Overall F1-score: 1.0\n",
            "-----------------------------\n",
            "Accuracy per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "Precision per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "Recall per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "F1 score per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[53  0  0  0]\n",
            " [ 0 56  0  0]\n",
            " [ 4  0 65  0]\n",
            " [ 0  0  0 78]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.984375\n",
            "Overall Precision: 0.98438\n",
            "Overall Recall: 0.98438\n",
            "Overall F1-score: 0.98438\n",
            "-----------------------------\n",
            "Accuracy per class: ['1.00000', '1.00000', '0.94203', '1.00000']\n",
            "Precision per class: ['0.92982', '1.00000', '1.00000', '1.00000']\n",
            "Recall per class: ['1.00000', '1.00000', '0.94203', '1.00000']\n",
            "F1 score per class: ['0.96364', '1.00000', '0.97015', '1.00000']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[52  0  1  0]\n",
            " [ 0 36  0  0]\n",
            " [ 4  0 40  1]\n",
            " [ 0  0  0 60]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9690721649484536\n",
            "Overall Precision: 0.96907\n",
            "Overall Recall: 0.96907\n",
            "Overall F1-score: 0.96907\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.98113', '1.00000', '0.88889', '1.00000']\n",
            "Precision per class: ['0.92857', '1.00000', '0.97561', '0.98361']\n",
            "Recall per class: ['0.98113', '1.00000', '0.88889', '1.00000']\n",
            "F1 score per class: ['0.95413', '1.00000', '0.93023', '0.99174']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_custom_eval(model4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve2cOlWgdjn3",
        "outputId": "3c7bc523-bc75-47ca-a686-96bfff6e869b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 3s 160ms/step\n",
            "2/2 [==============================] - 0s 112ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "2/2 [==============================] - 0s 107ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 131ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 1s 104ms/step\n",
            "(array([1., 1., 1., 1.]), array([1., 1., 1., 1.]), array([1., 1., 1., 1.]), array([188, 129, 140, 218]))\n",
            "2/2 [==============================] - 0s 112ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 109ms/step\n",
            "2/2 [==============================] - 0s 111ms/step\n",
            "(array([0.97222222, 1.        , 0.92727273, 1.        ]), array([0.95890411, 0.98076923, 0.98076923, 0.98734177]), array([0.96551724, 0.99029126, 0.95327103, 0.99363057]), array([73, 52, 52, 79]))\n",
            "2/2 [==============================] - 0s 159ms/step\n",
            "2/2 [==============================] - 0s 117ms/step\n",
            "2/2 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "(array([0.93333333, 1.        , 0.97619048, 1.        ]), array([0.97674419, 1.        , 0.93181818, 1.        ]), array([0.95454545, 1.        , 0.95348837, 1.        ]), array([43, 35, 44, 72]))\n",
            "<========== Train ==========>\n",
            "-----------------------------\n",
            "[[188   0   0   0]\n",
            " [  0 129   0   0]\n",
            " [  0   0 140   0]\n",
            " [  0   0   0 218]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 1.0\n",
            "Overall Precision: 1.0\n",
            "Overall Recall: 1.0\n",
            "Overall F1-score: 1.0\n",
            "-----------------------------\n",
            "Accuracy per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "Precision per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "Recall per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "F1 score per class: ['1.00000', '1.00000', '1.00000', '1.00000']\n",
            "-----------------------------\n",
            "<========== Val ============>\n",
            "-----------------------------\n",
            "[[70  0  3  0]\n",
            " [ 1 51  0  0]\n",
            " [ 1  0 51  0]\n",
            " [ 0  0  1 78]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.9765625\n",
            "Overall Precision: 0.97656\n",
            "Overall Recall: 0.97656\n",
            "Overall F1-score: 0.97656\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.95890', '0.98077', '0.98077', '0.98734']\n",
            "Precision per class: ['0.97222', '1.00000', '0.92727', '1.00000']\n",
            "Recall per class: ['0.95890', '0.98077', '0.98077', '0.98734']\n",
            "F1 score per class: ['0.96552', '0.99029', '0.95327', '0.99363']\n",
            "-----------------------------\n",
            "<========== Test ===========>\n",
            "-----------------------------\n",
            "[[42  0  1  0]\n",
            " [ 0 35  0  0]\n",
            " [ 3  0 41  0]\n",
            " [ 0  0  0 72]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.979381443298969\n",
            "Overall Precision: 0.97938\n",
            "Overall Recall: 0.97938\n",
            "Overall F1-score: 0.97938\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.97674', '1.00000', '0.93182', '1.00000']\n",
            "Precision per class: ['0.93333', '1.00000', '0.97619', '1.00000']\n",
            "Recall per class: ['0.97674', '1.00000', '0.93182', '1.00000']\n",
            "F1 score per class: ['0.95455', '1.00000', '0.95349', '1.00000']\n",
            "-----------------------------\n",
            "<===========================>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "After carefully analyzing the results, it becomes evident that each combination yields impressive overall accuracies.\n",
        "\n",
        "For the training subset, all combinations, except for model1 (freezing enabled, no extra Dense Layer), achieve near-perfect accuracies, hovering around 1.0. Model1 achieves a commendable accuracy of approximately 99%.\n",
        "\n",
        "Shifting our focus to the test set, model1, model3, and model4 all achieve accuracies of around 97%. These results indicate that whether we make the model trainable with no extra dense layers or vice versa, we end up with the same overall accuracy of 97%. However, enabling both the trainability of the model and the utilization of the extra dense layer results in a slight 1% increase in accuracy (model4).\n",
        "\n",
        "On the other hand, the combination that does not enable either of the two options (model2) exhibits the poorest performance on the test set, with an accuracy of approximately 91%.\n",
        "\n",
        "The same goes for the validation set. Apart from model2, which once again delivers the lowest accuracy, the remaining combinations achieve an overall accuracy of approximately 97%, with minor variations of around ±1% among the models.\n",
        "\n"
      ],
      "metadata": {
        "id": "ms17C6Jz_Em_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the models with external images \n",
        "\n",
        "In the final phase, we put our models to the test using a brand new dataset consisting of photos captured from our phone.\n",
        "\n",
        "We handpicked a total of 12 photos, with 3 images representing each class of our classification task.\n",
        "\n",
        "Our objective is to assess the performance of three models: the simple CNN model, the more complex one, and the pretrained InceptionV3 model (with trainability enabled and no extra layers added)."
      ],
      "metadata": {
        "id": "4sjguV5O-a3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "myphotos_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    myphotos_path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    seed=1,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "\n",
        "val_batches = tf.data.experimental.cardinality(myphotos_ds)\n",
        "print(val_batches)\n",
        "\n",
        "myphotos_ds.class_names\n",
        "NUM_CLASSES = len(myphotos_ds.class_names)\n",
        "print(myphotos_ds.class_names\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0pKYxHe_o6N",
        "outputId": "9ea0f45e-4181-4f27-8023-f026e0134a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 files belonging to 4 classes.\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "['cloudy', 'rain', 'shine', 'sunrise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Evaluating the simple model"
      ],
      "metadata": {
        "id": "wOw2jJ4kFoVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm_myphotos = confusion_matrix(cnn_simple_model, \"myphotos\")\n",
        "print(\"<========== My Photos ==========>\")\n",
        "calculate_metrics(cm_myphotos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEpCsNVPC9bN",
        "outputId": "37ffa79c-eab5-4db4-8051-d86073c8e4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "<========== My Photos ==========>\n",
            "-----------------------------\n",
            "[[3 0 0 0]\n",
            " [1 2 0 0]\n",
            " [0 1 2 0]\n",
            " [2 0 0 1]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.6666666666666666\n",
            "Overall Precision: 0.66667\n",
            "Overall Recall: 0.66667\n",
            "Overall F1-score: 0.66667\n",
            "-----------------------------\n",
            "Accuracy per class: ['1.00000', '0.66667', '0.66667', '0.33333']\n",
            "Precision per class: ['0.50000', '0.66667', '1.00000', '1.00000']\n",
            "Recall per class: ['1.00000', '0.66667', '0.66667', '0.33333']\n",
            "F1 score per class: ['0.66667', '0.66667', '0.80000', '0.50000']\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the complex model"
      ],
      "metadata": {
        "id": "AFgq-rJhFuFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm_myphotos = confusion_matrix(cnn_complex_model, \"myphotos\")\n",
        "print(\"<========== My Photos ==========>\")\n",
        "calculate_metrics(cm_myphotos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6seW6RfGHLlv",
        "outputId": "fad8e6c5-d21f-4e27-991e-8392b87130ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "<========== My Photos ==========>\n",
            "-----------------------------\n",
            "[[2 0 0 1]\n",
            " [0 3 0 0]\n",
            " [0 2 1 0]\n",
            " [0 0 0 3]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.75\n",
            "Overall Precision: 0.75\n",
            "Overall Recall: 0.75\n",
            "Overall F1-score: 0.75\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.66667', '1.00000', '0.33333', '1.00000']\n",
            "Precision per class: ['1.00000', '0.60000', '1.00000', '0.75000']\n",
            "Recall per class: ['0.66667', '1.00000', '0.33333', '1.00000']\n",
            "F1 score per class: ['0.80000', '0.75000', '0.50000', '0.85714']\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the pretrained InceptionV3 model"
      ],
      "metadata": {
        "id": "5gdFRPYDF0Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myphotos_ds = myphotos_ds.map(preprocess_func)\n",
        "cm_myphotos = confusion_matrix(model3, \"myphotos\")\n",
        "print(\"<========== My Photos ==========>\")\n",
        "calculate_metrics(cm_myphotos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCiVOq5BJgDa",
        "outputId": "31e796a4-af8f-40fe-d713-59e5cf57ab20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "<========== My Photos ==========>\n",
            "-----------------------------\n",
            "[[2 0 1 0]\n",
            " [0 2 1 0]\n",
            " [0 0 3 0]\n",
            " [0 0 0 3]]\n",
            "-----------------------------\n",
            "Overall Accuracy: 0.8333333333333334\n",
            "Overall Precision: 0.83333\n",
            "Overall Recall: 0.83333\n",
            "Overall F1-score: 0.83333\n",
            "-----------------------------\n",
            "Accuracy per class: ['0.66667', '0.66667', '1.00000', '1.00000']\n",
            "Precision per class: ['1.00000', '1.00000', '0.60000', '1.00000']\n",
            "Recall per class: ['0.66667', '0.66667', '1.00000', '1.00000']\n",
            "F1 score per class: ['0.80000', '0.80000', '0.75000', '1.00000']\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "Upon analyzing the results, it becomes apparent that the performance of the models varies across the different architectures.\n",
        "\n",
        "The simple model exhibits the lowest accuracy (66%), which is considerably lower compared to its previous evaluation on the test and validation sets. The major challenge lies in differentiating between the sunrise and cloudy classes, as a significant number of sunrise images were mistakenly classified as cloudy.\n",
        "\n",
        "The complex model performs slightly better (75%) than the simple architecture, showing an improvement compared to their previous comparison. However, it struggles with classifying images in the \"shine\" category, with a substantial number of images being misclassified as \"rain.\"\n",
        "\n",
        "In contrast, the pretrained model achieves the best accuracy (83%) and emerges as a reliable performer in this scenario.\n",
        "\n",
        "### Comments\n",
        "Considering the custom labeling we applied to our own photos, it is evident that both the simple and complex models are sensitive to the quality of the training data. This sensitivity stems primarily from the limited size of our training dataset. Even slight variations in the images can have a significant impact on their accuracies, revealing their limitations in making precise predictions.\n",
        "\n",
        "In contrast, the pretrained model, having been pretrained on the ImageNet dataset, showcases promising performance in our classification problem. It demonstrates robustness and appears to be less influenced by the quality of our training data.\n",
        "\n",
        "In conclusion, for real-world applications, it would be advisable to place trust in the pretrained model while disregarding the other architectures. However, the ideal scenario would involve training the model on a more extensive dataset than the one we utilized, allowing for improved generalization and better performance across various situations."
      ],
      "metadata": {
        "id": "iVVMHYG2Gihf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}